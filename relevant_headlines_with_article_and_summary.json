[{"id_field": "57a57dbf-93d0-4fd1-b2f7-a7635b426a14", "embed_text": "Binance Searches for Alleged Source Code Leaker - The world\\u2019s largest crypto exchange is on the hunt for a user it says publicly posted the keys to its crypto kingdom. In court documents first filed April 7, Binance asked the court to grant a subpoena against GitHub, trying to find any information about a user whom the company said had leaked excerpts of the Binance\\u2026Read more...", "title": "Binance Searches for Alleged Source Code Leaker", "description": "The world\\u2019s largest crypto exchange is on the hunt for a user it says publicly posted the keys to its crypto kingdom. In court documents first filed April 7, Binance asked the court to grant a subpoena against GitHub, trying to find any information about a user whom the company said had leaked excerpts of the Binance\\u2026Read more...", "link": "https://gizmodo.com/binance-source-code-crypto-cz-1850323685", "pubDate": "Tue, 11 Apr 2023 17:46:00 +0000", "feed_title": "Gizmodo", "feed_link": "https://gizmodo.com", "article": "Binance Searches for Alleged Source Code LeakerThe A.V. ClubDeadspinGizmodoJalopnikJezebelKotakuQuartzThe RootThe TakeoutThe OnionThe InventorySend us a Tip!ShopSubscribeThe Future Is HereWe may earn a commission from links on this pageSearchHomeLatestNewsGadgetsScienceEartherio9AISpaceEn Espa\u00f1olVideoCryptocurrenciesBinance Searches for Alleged Source Code LeakerBinance is asking the court for a subpoena to find out more about a GitHub user the crypto exchange alleges posted portions of its source code online.ByKyle BarrPublishedApril 11, 2023We may earn a commission from links on this page.Binance asked a court for a subpoena on a person it said had leaked parts of its source code online.Image: T. Schneider (Shutterstock)The world\u2019s largest crypto exchange is on the hunt for a user it says publicly posted the keys to its crypto kingdom. In court documents first filed April 7, Binance asked the court to grant a subpoena against GitHub, trying to find any information about a user whom the company said had leaked excerpts of the Binance source code online. WatchMaking the Facebook Papers Public\n\n\n\n\n\nCCShare SubtitlesOffEnglishShare this VideoFacebookTwitterEmailRedditLinkview videoMaking the Facebook Papers PublicSteam Deck Video ReviewMarch 7, 2022Why Banning TikTok Won\u2019t Protect Our PrivacyMarch 2, 2023Matthew Keys at The Desk first reported on the apparent code leak. According to an email chain provided in the court docs, the crypto exchange originally sent Microsoft-owned GitHub a notice under the Digital Millennium Copyright Act in late March, saying a user going by \u201cbonald\u201d posted the exchange\u2019s copyrighted code. The code repository company responded saying that Binance\u2019s supposed data was \u201cno longer available.\u201dAdvertisementAccording to the docs, Melanie Peker, senior legal council for Binance, asked the court to sign a subpoena that would supply the exchange bonald\u2019s real name, address, phone number, email, and any other identifying information. The GitHub page Binance links to in its original email no longer exists. The bonald GitHub account named in the court documents does not contain any mention of Binance code. The account has few followers, and according to the user\u2019s activity he hasn\u2019t posted any other code repositories since late last year. It\u2019s unclear what portions of Binance\u2019s source code appeared online and how many people downloaded those files before they were taken down.AdvertisementAdvertisementThe crypto exchange has yet to publicly comment on the alleged\u00a0code leak, and the courts have yet to grant Binance its subpoena, according to current court filings. Gizmodo reached out to both GitHub and Binance for comment, but we didn\u2019t immediately hear back.\u00a0As the world\u2019s largest crypto exchange by trading volume, Binance has every reason to fear any amount of its source code getting leaked online. Major crypto heists have racked up millions of dollars in damage in the first three months of 2023. Hackers pilfered $197 million from crypto lending protocol Euler Finance last month. Though more and more hacks have targeted DeFi projects, especially in 2022, Binance itself has been the target of hacks. Last October, it was hit with an exploit that created crypto out of thin air. Malicious actors have also been caught deepfaking Binance execs to try and gain access to users accounts.AdvertisementAround the time Binance sent its DMCA notice, the company was already reeling from a new regulatory lawsuit. The U.S. Commodities Futures Trading Commission alleged Binance had been illegally growing its user base in the country without registering its tokens. Binance CEO Changpeng Zhao argued that his company blocks U.S. users from using the wider Binance platform, and directs them to use the company\u2019s U.S. arm Binance.US.Binance is not the only online entity finding itself chasing after source code leaks. Twitter also asked the court to issue a subpoena to GitHub to learn who leaked portions of its source code. The courts granted Twitter\u2019s subpoena request shortly after filing, so Binance could be hoping to receive the same treatment.Start the discussion\n\n", "summary": "\nBinance, the world's largest cryptocurrency exchange, has requested a court subpoena to identify a user who allegedly leaked portions of its source code online. The code was originally posted to a GitHub page and has since been taken down, however Binance is seeking more information in order to identify the leaker."}, {"id_field": "238cf8c5-7291-45d9-8e0a-543fe7191ed0", "embed_text": "'Thirsty' AI: Training ChatGPT Required Enough Water to Fill a Nuclear Reactor's Cooling Tower, Study Finds - Popular large language models (LLMs) like OpenAI\\u2019s ChatGPT and Google\\u2019s Bard are energy intensive, requiring massive server farms to provide enough data to train the powerful programs. Cooling those same data centers also makes the AI chatbots incredibly thirsty. New research suggests training for GPT-3 alone consumed\\u2026Read more...", "title": "'Thirsty' AI: Training ChatGPT Required Enough Water to Fill a Nuclear Reactor's Cooling Tower, Study Finds", "description": "Popular large language models (LLMs) like OpenAI\\u2019s ChatGPT and Google\\u2019s Bard are energy intensive, requiring massive server farms to provide enough data to train the powerful programs. Cooling those same data centers also makes the AI chatbots incredibly thirsty. New research suggests training for GPT-3 alone consumed\\u2026Read more...", "link": "https://gizmodo.com/chatgpt-ai-water-185000-gallons-training-nuclear-1850324249", "pubDate": "Tue, 11 Apr 2023 19:45:00 +0000", "feed_title": "Gizmodo", "feed_link": "https://gizmodo.com", "article": "Training ChatGPT AI Required 185,000 Gallons of Water: StudyThe A.V. ClubDeadspinGizmodoJalopnikJezebelKotakuQuartzThe RootThe TakeoutThe OnionThe InventorySend us a Tip!ShopSubscribeThe Future Is HereWe may earn a commission from links on this pageSearchHomeLatestNewsGadgetsScienceEartherio9AISpaceEn Espa\u00f1olVideoArtificial Intelligence'Thirsty' AI: Training ChatGPT Required Enough Water to Fill a Nuclear Reactor's Cooling Tower, Study FindsAn average user\u2019s conversational exchange with ChatGPT amounts to dumping a large bottle of fresh water out on the ground, new research says.ByMack DeGeurinUpdatedMay 10, 2023Comments (48)We may earn a commission from links on this page.Photo: Alberto Ortega (AP)Popular large language models (LLMs) like OpenAI\u2019s ChatGPT and Google\u2019s Bard are energy-intensive, requiring massive server farms to provide enough data to train the powerful programs. Cooling those same data centers also makes the AI chatbots incredibly thirsty. New research suggests training for GPT-3 alone consumed 185,000 gallons (700,000 liters) of water. An average user\u2019s conversational exchange with ChatGPT basically amounts to dumping a large bottle of fresh water out on the ground, according to a new study. Given the chatbot\u2019s unprecedented popularity, researchers fear all those spilled bottles could take a troubling toll on water supplies, especially amid historic droughts and looming environmental uncertainty in the US.WatchTop Takeaways from the Apple Event\n\n\n\n\n\nCCShare SubtitlesOffEnglishShare this VideoFacebookTwitterEmailRedditLinkview videoTop Takeaways from the Apple EventWhy Banning TikTok Won\u2019t Protect Our PrivacyMarch 2, 2023Mac Studio and Studio Display ReviewMarch 18, 2022Researchers from the University of California Riverside and the University of Texas Arlington published the AI water consumption estimates in a pre-print paper titled \u201cMaking AI Less \u2018Thirsty.\u2019\u201d The authors found the amount of clear freshwater required to train GPT-3 is equivalent to the amount needed to fill a nuclear reactor\u2019s cooling tower. OpenAI has not disclosed the length of time required to train GPT-3, complicating the researchers\u2019 estimations, but Microsoft, which has struck a multi-year, multi-billion-dollar partnership with the AI startup and built supercomputers for AI training, says that its latest supercomputer, which would require an extensive cooling apparatus, contains 10,000 graphics cards and over 285,000 processor cores, giving a glimpse into the vast scale of the operation behind artificial intelligence. That huge number of gallons could produce battery cells for 320 Teslas, or, put another way, ChatGPT, which came after GPT-3, would need to \u201cdrink\u201d a 500-milliliter water bottle in order to complete a basic exchange with a user consisting of roughly 25-50 questions.AdvertisementThe gargantuan number of gallons needed to train the AI model also assumes the training is happening in Microsoft\u2019s state-of-the-art US data center, built especially for OpenAI to the tune of tens of millions. If the data was trained in the company\u2019s less energy-efficient Asia data center, the report notes water consumption could be three times higher. The researchers expect these water requirements will only increase further with newer models, like the recently released GPT-4, which rely on a larger set of data parameters than their predecessors. \u201cAI models\u2019 water footprint can no longer stay under the radar,\u201d the researchers said. \u201cWater footprint must be addressed as a priority as part of the collective efforts to combat global water challenges.\u201dAdvertisementAdvertisementHow do chatbots use water?When calculating AI\u2019s water consumption, the researchers draw a distinction between water \u201cwithdrawal\u201d and \u201cconsumption.\u201d The first example is the practice of physically removing water from a river, lake, or other source, while consumption refers specifically to the loss of water by evaporation when it\u2019s used in data centers. The research on AI\u2019s water usage focuses primarily on the consumption part of that equation, where the water can\u2019t be recycled.AdvertisementAnyone who\u2019s spent a few seconds in a company server room knows you need to pack a sweater first. Server rooms are kept cool, typically between 50 and 80 degrees Fahrenheit to prevent equipment from malfunctioning. Maintaining that ideal temperature is a constant challenge because the servers themselves convert their electrical energy into heat. Cooling towers like the ones shown below are often deployed to try and counteract that heat and keep the rooms in their ideal temperature by evaporating cold water.Evaporative Cooling Towers in the Data Center ProcessCooling towers get the job done, but they require immense amounts of water to do so. The researchers estimate around a gallon of water is consumed for every kilowatt-hour expended in an average data center. Not just any type of water can be used, either. Data centers pull from clean, freshwater sources in order to avoid the corrosion or bacteria growth that can come with seawater. Freshwater is also essential for humidity control in the rooms. The researchers likewise hold data centers accountable for the water needed to generate the high amounts of electricity they consume, something the scientists called \u201coff-site indirect water consumption.\u201d AdvertisementGoogle data center imagesWater consumption issues aren\u2019t limited to OpenAI or AI models. In 2019, Google requested more than 2.3 billion gallons of water for data centers in just three states. The company currently has 14 data centers spread out across North America which it uses to power Google Search, its suite of workplace products, and more recently, its LaMDa and Bard large language models. LaMDA alone, according to the recent research paper, could require millions of liters of water to train, larger than GPT-3 because several of Google\u2019s thirsty data centers are housed in hot states like Texas; researchers issued a caveat with this estimation, though, calling it an \u201c approximate reference point.\u201dAdvertisementAside from water, new LLMs similarly require a staggering amount of electricity. A Stanford AI report released last week looks at differences in energy consumption among four prominent AI models, estimating OpenAI\u2019s GPT-3 released 502 metric tons of carbon during its training. Overall, the energy needed to train GPT-3 could power an average American\u2019s home for hundreds of years.\u201cThe race for data centers to keep up with it all is pretty frantic,\u201d Critical Facilities Efficiency Solution CEO Kevin Kent said in an interview with Time. \u201cThey can\u2019t always make the most environmentally best choices.\u201dAdvertisementClimate change and worsening droughts could amplify concerns over AI\u2019s water usageAlready, the World Economic Forum estimates some 2.2 million US residents lack water and basic indoor plumbing. Another 44 million live with \u201cinadequate\u201d water systems. Researchers fear a combination of climate change and increased US populations will make those figures even worse by the end of the century. By 2071, Stanford estimates nearly half of the country\u2019s 204 freshwater basins will be unable to meet monthly water demands. Many regions could reportedly see their water supplies cut by a third in the next 50 years.AdvertisementRising temperatures partially fueled by human activity have resulted in the American West recording its worst drought in 1,000 years which also threatens freshwater, though recent flooding rains have helped stave off some dire concerns. Water levels at reservoirs like Lake Mead have receded so far that they\u2019ve exposed decades old human remains. All of that means AI\u2019s hefty water demands will likely become a growing point of contention, especially if the tech is embedded into ever more sectors and services. Data requirements for LLMs are only getting larger, which means companies will have to find ways to increase their data centers\u2019 water efficiency. Researchers say there are some relatively clear ways to bring AI\u2019s water price tag down. For starters, where and when AI models are trained matters. Outside temperatures, for example, can affect the amount of water required to cool data centers. AI companies could hypothetically train models at midnight when it\u2019s cooler or in a data center with better water efficiency to cut down on usage. Chatbot users, on the other hand, could opt to engage with the modules during \u201cwater-efficient hours,\u201d much as municipal authorities encourage off-hours dishwasher use. Still, any of those demand-side changes will require greater transparency on the part of tech companies building these models, something the researchers say is in worryingly short supply.Advertisement\u201cWe recommend AI model developers and data center operators be more transparent,\u201d the researchers wrote. \u201cWhen and where are the AI models trained? What about the AI models trained and/or deployed in third-party colocation data centers or public clouds? Such information will be of great value to the research community and the general public.\u201dWant to know more about AI, chatbots, and the future of machine learning? Check out our full coverage of artificial intelligence, or browse our guides to The Best Free AI Art Generators and Everything We Know About OpenAI\u2019s ChatGPT.Show all 48 commentsContinue reading\n\n", "summary": "\nNew research suggests training for GPT-3, a popular large language model, required 185,000 gallons of water to complete. This equates to an average user's conversational exchange with the chatbot being equivalent to dumping a large bottle of fresh water on the ground."}, {"id_field": "ee347254-8619-4dd4-bb8a-99371d91a332", "embed_text": "Free AI Video Generators Are Nearing a Crucial Tipping Point - Video memes made with algorithms are suddenly everywhere. Their sudden proliferation may herald an imminent explosion in the technology's capability.", "title": "Free AI Video Generators Are Nearing a Crucial Tipping Point", "description": "Video memes made with algorithms are suddenly everywhere. Their sudden proliferation may herald an imminent explosion in the technology's capability.", "link": "https://www.wired.com/story/ai-video-generators-are-nearing-a-crucial-tipping-point/", "pubDate": "Thu, 06 Apr 2023 16:00:00 +0000", "feed_title": "Wired", "feed_link": "https://www.wired.com", "article": "Free AI Video Generators Are Nearing a Crucial Tipping Point | WIREDSkip to main contentOpen Navigation MenuMenuStory SavedTo revist this article, visit My Profile, then View saved stories.Close AlertAI Video Generators Are Nearing a Crucial Tipping PointBackchannelBusinessCultureGearIdeasScienceSecurityPrime DayMoreChevronStory SavedTo revist this article, visit My Profile, then View saved stories.Close AlertSign InSearchSearchBackchannelBusinessCultureGearIdeasScienceSecurityPrime DayPodcastsVideoArtificial IntelligenceClimateGamesNewslettersMagazineEventsWired InsiderJobsCouponsWill KnightBusinessApr 6, 2023 12:00 PMAI Video Generators Are Nearing a Crucial Tipping PointVideo memes made with algorithms are suddenly everywhere. Their sudden proliferation may herald an imminent explosion in the technology's capability.Photograph: Jonathan Kitchen/Getty ImagesSave this storySaveSave this storySaveYou may have noticed some impressive video memes made with AI in recent weeks.\u00a0Harry Potter reimagined as a Balenciaga commercial\u00a0and nightmarish footage of\u00a0Will Smith eating spaghetti both recently went viral. They highlight how quickly AI\u2019s ability to create video is advancing, as well as how problematic some uses of the technology may be.Sign Up TodayThis is an edition of WIRED's Fast Forward newsletter, a weekly dispatch from the future by Will Knight, exploring AI advances and other technology set to change our lives.These videos remind me of the moment AI image-making tools became widespread last year, when programs like\u00a0Craiyon (formerly known as DALL-E Mini) let anyone conjure up recognizable, if crude and often surreal, images, such as\u00a0surveillance footage of babies robbing a gas station,\u00a0Darth Vadar courtroom sketches, and\u00a0Elon Musk eating crayons.\u00a0Craiyon was an open source knockoff of the then carefully restricted\u00a0DALL-E 2 image generator from\u00a0OpenAI, the company behind ChatGPT. The tool was the first to show AI\u2019s ability to take a text prompt and turn it into what looked like real photos and human-drawn illustrations. Since then, DALL-E has become open to everyone, and programs like\u00a0Midjourney and\u00a0Dream Studio have developed and honed similar tools, making it relatively trivial to craft complex and realistic images with a few taps on a keyboard.As engineers have tweaked the algorithmic knobs and levers behind these image generators, added more training data, and paid for more\u00a0GPU chips to run everything, these image-making tools have become incredibly good at faking reality. To take a few examples from a subreddit dedicated to strange AI images, check out\u00a0Alex Jones at a gay pride parade or the\u00a0Ark of the Covenant at a yard sale.\u00a0Widespread access to this technology, and its sophistication, forces us to rethink how we view online imagery, as was highlighted after AI-made images purporting to show\u00a0Donald Trump\u2019s arrest went viral last month. The incident led Midjourney to announce that it would no longer offer a free trial of its service\u2014a fix that might deter some cheapskate bad actors but leaves the broader problem untouched.As WIRED\u2019s\u00a0Amanda Hoover writes this week, algorithms still struggle\u00a0to generate convincing video from a prompt. Creating many individual frames is computationally expensive, and as today\u2019s jittering and sputtering videos show, it is hard for algorithms to maintain enough coherence between them to produce a video that makes sense.\u00a0AI tools are, however, getting a lot more adept at editing videos. The Balenciaga meme, along with versions referencing\u00a0Friends and\u00a0Breaking Bad, were made by combining a few different AI tools, first to generate still images and then to add simple animation effects. But the end result is still impressive.\u00a0Most PopularGearWhy We Don\u2019t Recommend Ring CamerasAdrienne SoCultureAfter Threads, There Has to Be a \u2018New Twitter\u2019 MoratoriumKate KnibbsGearWIRED\u2019s Favorite \u2018Buy It for Life\u2019 GearParker HallGearThe Best Early Prime Day DealsLouryn StrampeRunway ML, a startup that\u2019s developing AI tools for professional image and video creation and editing, this week launched a\u00a0new more efficient technique for applying stylistic changes to videos. I used it to create this dreamlike footage of my cat, Leona, walking through a \u201ccloudscape\u201d from an existing video in just a few minutes.Video: Will Knight/RunwayDifferent machine learning techniques open new possibilities. A company called\u00a0Luma AI, for instance, is using a technique known as\u00a0neural radiance fields to turn 2D photographs into detailed 3D scenes. Feed a few snapshots into the company\u2019s app, and you\u2019ll have a\u00a0fully interactive 3D scene to play with.\u00a0These clips suggestt we are at an inflection point for AI video making. As with AI image generation, a growing rush of memes could be followed by significant improvements in the quality and controllability of AI videos that lodge the technology in all sorts of places. AI may well become a muse for some auteurs. Runway\u2019s tools were\u00a0used by the visual effects artists working on the Oscar-winning\u00a0Everything Everywhere All At Once.\u00a0Darren Aronofsky, director of\u00a0The Whale,\u00a0Black Swan, and\u00a0Pi is also\u00a0a fan of Runway.\u00a0But you only need to look at how advanced images from Midjourney and Dream Studio are now to sense where AI video is heading\u2014and how difficult it may become to distinguish real clips from fake ones. Of course, people can already manipulate videos with existing technology, but it\u2019s still relatively expensive and difficult to pull off.The rapid advances in generative AI may prove dangerous in an era when social media has been weaponized and\u00a0deepfakes are propagandists' playthings. As\u00a0Jason Parham wrote for WIRED this week, we also need to seriously consider how generative AI can\u00a0recapture and repurpose ugly stereotypes.For now, the instinct to trust video clips is mostly reliable, but it might not be long before the footage we see is less solid and truthful than it once was.Get More From WIRED\ud83d\udce9 Don\u2019t miss our biggest stories, delivered to your inbox every day\ud83c\udfa7 Our new podcast wants you to Have a Nice FutureThe night 17 million military records went up in smokeThe AI protest group campaigning against human extinctionHow your new car tracks youA grid collapse would make a heat wave far deadlierWIRED\u2019s favorite \u201cbuy it for life\u201d gear\ud83c\udf32 Our Gear team has branched out with a new guide to the best sleeping pads and fresh picks for the best coolers and binocularsWill Knight is a senior writer for WIRED, covering artificial intelligence. He writes the the Fast Forward newsletter that explores how advances in AI and other emerging technology are set to change our lives\u2014sign up here. He was previously a senior editor at MIT Technology Review, where he wrote about... Read moreSenior WriterTwitterTopicsFast ForwardMore from WIREDThe Huge Power and Potential Danger of AI-Generated CodeProgramming can be faster when algorithms help out, but there is evidence AI coding assistants also make bugs more common.Will KnightThe Last AI Boom Didn't Kill Jobs. Feel Better?ChatGPT is stoking fears of mass layoffs, but a study of several EU countries found the deep-learning boom of the 2010s actually created job opportunities.Will KnightGenerative AI in Games Will Create a Copyright CrisisTitles like AI Dungeon are already using generative AI to generate in-game content. Nobody knows who owns it.Will BedingfieldGoogle DeepMind\u2019s CEO Says Its Next Algorithm Will Eclipse ChatGPTDemis Hassabis says the company is working on a system called Gemini that will draw on techniques that powered AlphaGo to a historic victory over a Go champion in 2016.Will KnightHow to Tackle AI\u2014and Cheating\u2014in the ClassroomWhat one educator wants students, teachers, and everyone else to know about the ethics of using of AI in education.Christina WymanMeet the Humans Trying to Keep Us Safe From AIAs artificial intelligence explodes, the field is expanding beyond the usual suspects\u2014and the usual motivations.Will KnightChatGPT Is Reshaping Crowd WorkAlthough some workers shun chatbot help, platforms are adopting policies or technology to deter use of AI\u2014potentially making crowd work more difficult.Caitlin\u00a0HarringtonMeet the AI Protest Group Campaigning Against Human ExtinctionFears that artificial intelligence might wipe us out have fueled the rise of groups like Pause AI. Their warnings aren\u2019t that far-fetched, experts say.Morgan MeakerWIRED is where tomorrow is realized. It is the essential source of information and ideas that make sense of a world in constant transformation. The WIRED conversation illuminates how technology is changing every aspect of our lives\u2014from culture to business, science to design. The breakthroughs and innovations that we uncover lead to new ways of thinking, new connections, and new industries.FacebookTwitterPinterestYouTubeInstagramTiktokMore From WIREDSubscribeNewslettersFAQWired StaffPress CenterCouponsEditorial StandardsPrime DayArchiveContactAdvertiseContact UsCustomer CareJobsRSSAccessibility HelpCond\u00e9 Nast StoreDo Not Sell My Personal Info\u00a9 2023 Cond\u00e9 Nast. All rights reserved. Use of this site constitutes acceptance of our User Agreement and Privacy Policy and Cookie Statement and Your California Privacy Rights. WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Cond\u00e9 Nast. Ad ChoicesSelect international siteUnited StatesLargeChevronUKItaliaJap\u00f3n", "summary": "\nAI video-generating tools have become increasingly sophisticated, allowing anyone to create impressive video memes with a few taps on a keyboard. As AI video technology advances, the line between real and fake videos is becoming increasingly blurred, raising questions about the ethics of its use."}, {"id_field": "7989e956-65de-49f2-b63b-65cffd69f6b4", "embed_text": "AI Desperately Needs Global Oversight - As ChatGPT and its ilk continue to spread, countries need an independent board to hold AI companies accountable and limit harms.", "title": "AI Desperately Needs Global Oversight", "description": "As ChatGPT and its ilk continue to spread, countries need an independent board to hold AI companies accountable and limit harms.", "link": "https://www.wired.com/story/ai-desperately-needs-global-oversight/", "pubDate": "Thu, 06 Apr 2023 13:00:00 +0000", "feed_title": "Wired", "feed_link": "https://www.wired.com", "article": "AI Desperately Needs Global Oversight | WIREDSkip to main contentOpen Navigation MenuMenuStory SavedTo revist this article, visit My Profile, then View saved stories.Close AlertAI Desperately Needs Global OversightBackchannelBusinessCultureGearIdeasScienceSecurityPrime DayMoreChevronStory SavedTo revist this article, visit My Profile, then View saved stories.Close AlertSign InSearchSearchBackchannelBusinessCultureGearIdeasScienceSecurityPrime DayPodcastsVideoArtificial IntelligenceClimateGamesNewslettersMagazineEventsWired InsiderJobsCouponsRumman ChowdhuryIdeasApr 6, 2023 9:00 AMAI Desperately Needs Global OversightAs ChatGPT and its ilk continue to spread, countries need an independent board to hold AI companies accountable and limit harms.Play/Pause ButtonPausePhoto-illustration: WIRED Staff; Getty ImagesSave this storySaveSave this storySaveEvery time you post a photo, respond on social media, make a website, or possibly even send an email, your data is scraped, stored, and used to train generative AI technology that can create text, audio, video, and images with just a few words. This has real consequences: OpenAI researchers studying the labor market impact of their language models estimated that approximately 80 percent of the US workforce could have at least 10 percent of their work tasks affected by the introduction of large language models (LLMs) like ChatGPT, while around 19 percent of workers may see at least half of their tasks impacted. We\u2019re seeing an immediate labor market shift with image generation, too. In other words, the data you created may be putting you out of a job.When a company builds its technology on a public resource\u2014the internet\u2014it\u2019s sensible to say that that technology should be available and open to all. But critics have noted that GPT-4 lacked any clear information or specifications that would enable anyone outside the organization to replicate, test, or verify any aspect of the model. Some of these companies have received vast sums of funding from other major corporations to create commercial products. For some in the AI community, this is a dangerous sign that these companies are going to seek profits above public benefit.WIRED OPINIONABOUTRumman Chowdhury is a Responsible AI Fellow at the Berkman Klein Center for Internet and Society at Harvard University and a visiting researcher at the Minderoo Centre for Technology and Democracy at the University of Cambridge. Previously, she was the Director of Machine Learning Ethics, Transparency, and Accountability at Twitter.Code transparency alone is unlikely to ensure that these generative AI models serve the public good. There is little conceivable immediate benefit to a journalist, policy analyst, or accountant (all \u201chigh exposure\u201d professions according to the OpenAI study) if the data underpinning an LLM is available. We increasingly have laws, like the Digital Services Act, that would require some of these companies to open their code and data for expert auditor review. And open source code can sometimes enable malicious actors, allowing hackers to subvert safety precautions that companies are building in. Transparency is a laudable objective, but that alone won\u2019t ensure that generative AI is used to better society.In order to truly create public benefit, we need mechanisms of accountability. The world needs a generative AI global governance body to solve these social, economic, and political disruptions beyond what any individual government is capable of, what any academic or civil society group can implement, or any corporation is willing or able to do. There is already precedent for global cooperation by companies and countries to hold themselves accountable for technological outcomes. We have examples of independent, well-funded expert groups and organizations that can make decisions on behalf of the public good. An entity like this is tasked with thinking of benefits to humanity. Let\u2019s build on these ideas to tackle the fundamental issues that generative AI is already surfacing.In the nuclear proliferation era after World War II, for example, there was a credible and significant fear of nuclear technologies gone rogue. The widespread belief that society had to act collectively to avoid global disaster echoes many of the discussions today around generative AI models. In response, countries around the world, led by the US and under the guidance of the United Nations, convened to form the International Atomic Energy Agency (IAEA), an independent body free of government and corporate affiliation that would provide solutions to the far-reaching ramifications and seemingly infinite capabilities of nuclear technologies. It operates in three main areas: nuclear energy, nuclear safety and security, and safeguards. For instance, after the Fukushima disaster in 2011 it provided critical resources, education, testing, and impact reports, and helped to ensure ongoing nuclear safety. However, the agency is limited: It relies on member states to voluntarily comply with its standards and guidelines, and on their cooperation and assistance to carry out its mission.In tech, Facebook\u2019s Oversight Board is one working attempt at balancing transparency with accountability. The Board members are an interdisciplinary global group, and their judgments, such as overturning a decision made by Facebook to remove a post that depicted sexual harassment in India, are binding. This model isn\u2019t perfect either; there are accusations of corporate capture, as the board is funded solely by Meta, albeit through an independent trust, and is primarily concerned with content takedowns.Most PopularGearWhy We Don\u2019t Recommend Ring CamerasAdrienne SoCultureAfter Threads, There Has to Be a \u2018New Twitter\u2019 MoratoriumKate KnibbsGearWIRED\u2019s Favorite \u2018Buy It for Life\u2019 GearParker HallGearThe Best Early Prime Day DealsLouryn StrampeHowever flawed, both of these examples provide a starting point for what an AI global governance body might look like. An organization like this should be a consolidated ongoing effort with expert advisory and collaborations, like the IAEA, rather than a secondary project for people with other full-time jobs. Like the Facebook Oversight Board, it should receive advisory input and guidance from industry, but have the capacity to make independent binding decisions that companies must comply with.This generative AI global governance body should be funded via unrestricted funds (in other words, no strings attached) by all of the companies engaged in at-scale generation and use of generative AI of any form. It should cover all aspects of generative AI models, including their development, deployment, and use as it relates to the public good. It should build upon tangible recommendations from civil society and academic organizations, and have the authority to enforce its decisions, including the power to require changes in the design or use of generative AI models, or even halt their use altogether if necessary. Finally, this group should address reparations for the sweeping changes that may come, job loss, a rise in misinformation, and the potential for inhibiting free and fair elections potentially among them. This is not a group for research alone; this is a group for action.Today, we have to rely on companies to do the right thing, but aligning the greater good with stakeholder incentives has proven to be insufficient. With this structure, the AI oversight group would be positioned to take action like corporations can, but with the purpose of public good. Here\u2019s one example of how. First, through secure data-sharing, it could do research currently conducted by these companies. The OpenAI economic harms paper, while admirable, should be the remit of an impartial third party rather than a corporation. Second, this group\u2019s job is not just to identify problems, but to experiment with novel ways to fix them. Using the \u201ctax\u201d that corporations pay to join, this group might set up an education or living support fund for displaced workers that people can apply for to supplement unemployment benefits, or a universal basic income based on income levels, regardless of employment status, or proportional payout compared to the data that could be attributed to you as a contributing member of digital society. Finally, based on collaboration with civil society, governments, and the companies themselves, it would be empowered to take action, perhaps requiring companies to slow down implementation in particularly high-impact industries and support job transition programs.The issues that generative AI developments raise are difficult to grapple with meaningfully, and as a society we currently lack the means to address them at the speed and scale at which new technology is being thrust upon us. Generative AI companies have the responsibility to entrust an independent body speaking on behalf of the world to make critical decisions on governance and impact.Updated 4-7-2023, 12:30 PM ET: An earlier version of this article stated that the Facebook Oversight Board can only hear cases that Facebook itself refers. and that it does not address systemic issues like algorithms or moderation policies. In fact, users can also make appeals, and the Board has issued broader policy recommendations. Additionally, this article has been updated to clarify that the Facebook Oversight Board is funded by Meta through an independent trust.WIRED Opinion publishes articles by outside contributors representing a wide range of viewpoints. Read more opinions here, and see our submission guidelines here. Submit an op-ed at opinion@wired.com.Get More From WIRED\ud83d\udce9 Don\u2019t miss our biggest stories, delivered to your inbox every day\ud83c\udfa7 Our new podcast wants you to Have a Nice FutureThe night 17 million military records went up in smokeThe AI protest group campaigning against human extinctionHow your new car tracks youA grid collapse would make a heat wave far deadlierWIRED\u2019s favorite \u201cbuy it for life\u201d gear\ud83c\udf32 Our Gear team has branched out with a new guide to the best sleeping pads and fresh picks for the best coolers and binocularsRumman Chowdhury is a Responsible AI Fellow at the Berkman Klein Center for Internet and Society at Harvard University and a visiting researcher at the Minderoo Centre for Technology and Democracy at the University of Cambridge. Previously, she was the Director of Machine Learning Ethics, Transparency, and Accountability at Twitter.Op-ed contributorTopicsartificial intelligenceethicsTech Policy and LawgovernmentMore from WIREDMilitary AI\u2019s Next Frontier: Your Work ComputerSpycraft developed by defense contractors are now being sold to employers to identify labor organizing. Regulators must step up to protect workers\u2019 privacy.Gabriel GrillBetter Government Tech Is PossibleThere's so much potential for the government to use technology to improve the lives of citizens. It starts with acknowledging the importance of training.Beth Simone NoveckHow Microsoft Excel Tries to Rebrand Work as ExcitementThis banal type of office software has a long history of trying to pass itself off as whimsical.\u00a0Benjamin Charles Germain LeeLet People Collect Sperm From the DeadIt may now be easier to get pregnant using the sperm of a deceased loved one. The practice is controversial\u2014but it\u2019s not inherently wrong.Catriona MortonGive Every AI a Soul\u2014or ElseTo solve the \u201ccrisis\u201d in artificial intelligence, AI beings must say, \u201cI am me.\u201dDavid BrinBeware the Digital WhiteboardA new office tool is infecting thought and communication with the worst symptoms of design thinking.Megan MarzChatGPT Is Unoriginal\u2014and Exactly What Humans NeedThe technology can help cut through buzzwordy \u201csolutions\u201d and serve as a shortcut for jumpstarting creativity. Dana KaroutInmates Need Internet to Prepare for Life After PrisonGiving inmates access to the technology they'll rely on when they reenter society is a key to reducing recidivism.Luke Elliott SommerWIRED is where tomorrow is realized. It is the essential source of information and ideas that make sense of a world in constant transformation. The WIRED conversation illuminates how technology is changing every aspect of our lives\u2014from culture to business, science to design. The breakthroughs and innovations that we uncover lead to new ways of thinking, new connections, and new industries.FacebookTwitterPinterestYouTubeInstagramTiktokMore From WIREDSubscribeNewslettersFAQWired StaffPress CenterCouponsEditorial StandardsPrime DayArchiveContactAdvertiseContact UsCustomer CareJobsRSSAccessibility HelpCond\u00e9 Nast StoreDo Not Sell My Personal Info\u00a9 2023 Cond\u00e9 Nast. All rights reserved. Use of this site constitutes acceptance of our User Agreement and Privacy Policy and Cookie Statement and Your California Privacy Rights. WIRED may earn a portion of sales from products that are purchased through our site as part of our Affiliate Partnerships with retailers. The material on this site may not be reproduced, distributed, transmitted, cached or otherwise used, except with the prior written permission of Cond\u00e9 Nast. Ad ChoicesSelect international siteUnited StatesLargeChevronUKItaliaJap\u00f3n", "summary": "\nAI companies are increasingly using large language models (LLMs) and image generation technology to scrape and store user data for their models, leading to potential job loss and other impacts to the global economy. An independent global governance body is needed to monitor and regulate these companies and to ensure the public good is taken into account."}, {"id_field": "3ec5b656-3988-4e99-a283-4a9fa5e3164e", "embed_text": "https://t.co/a2HUzagPtI", "title": "https://t.co/a2HUzagPtI", "description": "null", "link": "https://twitter.com/i/web/status/1645568467800846337", "pubDate": "null", "feed_title": "Twitter Feed", "feed_link": "https://twitter.com"}]